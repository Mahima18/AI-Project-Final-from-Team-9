\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{mathpazo}
\usepackage{fullpage}
\usepackage{hyphenat}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{setspace}
\onehalfspacing
\graphicspath{ {./images/} }

\newcommand{\reviewtitle}{%                                              
    \usebox{\btitle}\\\\ 
    \vspace{.5\baselineskip}
    \begin{minipage}{.5\linewidth}
      \begin{center}
        \usebox{\bmember}\\
        \usebox{\bstudent}\\
        \usebox{\bpartner}\\
        Under the supervision of \usebox{\bsemester}\\
      \end{center}
    \end{minipage}\
    \begin{minipage}{1.0\linewidth}
      \begin{center}
        \usebox{\textbf{Mr. Rishabh Kaushal}}\\\\
      \end{center}
    \end{minipage}
    \begin{center}
      \usebox{\breview} \usebox{Assistant Professor}\\
      Department of Information Technology\\\\\\
      \includegraphics[width=2.51667in,height=1.76667in]{logo.jpeg}\\
      \textbf{Department of Information Technology}\\
      \textnormal{Indira Gandhi Delhi Technical University for Women}\\
      \textnormal{Kashmere Gate, Delhi - 110006}\\
    \end{center} 
  }
  
\title{DRIVER DROWSINESS DETECTION}

\begin{document}

\begin{center}
\textnormal{Submitted by}

\textnormal{TEAM 9}

\textbf{Millennium-01301032017}

\textbf{A.Vaishnavi -05601032017}

\textbf{Rashmita Yadav- 05801032017}

\textbf{Rhea Prasad- 06101032017}

\textbf{Mahima Patel- 06201032017}\\\\
\reviewtitle

\end{center}

\pagebreak

\textbf{{Contents}}

\textbf{ABSTRACT 4}

\textbf{1 INTRODUCTION 5}

1.1 Problem
Statement\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots.5

1.1.1
Objective\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots6

\textbf{2 LITERATURE REVIEW 7}

2.1 What is
Drowsiness?................................................................................................................................7

2.2
Questionaries\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots.7

\textbf{3 COUNTER MEASURES 10}

3.1 Vehicle based
measures\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots10

3.2 Physiological
measures\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots.10

3.3 Subjective
Method\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots11

\textbf{4 PROPOSED METHODOLOGY 13}

4.1 Behavioral
measures\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots..13

4.2 Data Source and
Pre-Processing\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots.13

4.3 Feature
Computation\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots.14

4.3.1 Feature
Extraction\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots14

4.3.2 Feature
Normalization\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots..17

\textbf{5 DATA EXPLORATION 18}

5.1
Metrics\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots.18

5.2
Visualization\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots.18

5.2.1 Feature
Implementation\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots.19

5.2.2 Normalized Feature
Value\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots.\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots.19

5.2.3 State
Prediction\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots.19

\textbf{6 SYSTEM DESCRIPTION 20}

6.1 Tools
Used\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots.20

6.2 Technology
Used\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots..20

\textbf{7 ALGORITHMS 22}

7.1 Basic Classification
Methods\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots.\ldots\ldots\ldots\ldots\ldots22

7.1.1
Description\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots22

7.2
CNN\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots.23

7.2.1
Description\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots23

7.2.2 CNN
Parameters\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots23

7.2.3 CNN Model
Design\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots..24

7.3
LSTM\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots..24

7.3.1
Description\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots24

7.3.2 LSTM
Parameters\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots.25

7.3.3 LSTM Model
Design\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots.\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots26

7.3.4 Graphical Representation of
Results\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots.\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots..27

7.3.5
Conclusion\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots.28

7.4 Comparison of
Algorithms\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots..30

7.4.1 Accuracy
Curve\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots31

7.4.2 ROC
Curve\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots..31

7.4.3 Calibration
Curve\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots..32

\textbf{8 CONCLUSION 34}

8.1
Limitations\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots.\ldots34

8.2 Future
Work\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots\ldots.35

\textbf{ABSTRACT}

In recent years driver fatigue is one of the major causes of vehicle
accidents in the world. A direct way of measuring driver fatigue is
measuring the state of the driver i.e. drowsiness. So it is very
important to detect the drowsiness of the driver to save life and
property. This project is aimed towards developing a prototype of
drowsiness detection system. This system is a real time system which
captures image continuously and measures the state of the eye according
to the specified algorithm.

Though there are several methods for measuring the drowsiness but this
approach is completely non-intrusive which does not affect the driver in
any way, hence giving the exact condition of the driver. This system
works by monitoring the eyes and mouth of the driver.

The priority is on improving the safety of the driver without being
obtrusive. In this project the eye and mouth landmarks of the driver is
detected. If the driver's eyes remain closed for more than a certain
period of time, the driver is said to be drowsy. The programming for
this is done in OpenCV for the detection of facial features.

\textbf{CHAPTER 1}

\textbf{INTRODUCTION}

Driver drowsiness detection is a car safety technology which prevents
accidents when the driver is getting drowsy. Various studies have
suggested that around 20\% of all road accidents are fatigue-related, up
to 50\% on certain roads. Driver fatigue is a significant factor in a
large number of vehicle accidents. Recent statistics estimate that
annually 1,200 deaths and 76,000 injuries can be attributed to fatigue
related crashes. The development of technologies for detecting or
preventing drowsiness at the wheel is a major challenge in the field of
accident avoidance systems. Because of the hazard that drowsiness
presents on the road, methods need to be developed for counteracting its
affects. Driver inattention might be the result of a lack of alertness
when driving due to driver drowsiness and distraction. Driver
distraction occurs when an object or event draws a person's attention
away from the driving task. Unlike driver distraction, driver drowsiness
involves no triggering event but, instead, is characterized by a
progressive withdrawal of attention from the road and traffic demands.
Both driver drowsiness and distraction, however, might have the same
effects, i.e., decreased driving performance, longer reaction time, and
an increased risk of crash involvement. shows the block diagram of
overall system. Based on Acquisition of video from the camera that is in
front of driver perform real-time processing of an incoming video stream
in order to infer the driver's level of fatigue if the drowsiness is
estimated.

\textbf{1.1 PROBLEM STATEMENT}

1 in 4 vehicle accidents are caused by drowsy driving and 1 in 25 adult
drivers report that they have fallen asleep at the wheel in the past 30
days. The scariest part is that drowsy driving isn't just falling asleep
while driving. Drowsy driving can be as small as a brief state of
unconsciousness when the driver is not paying full attention to the
road. Drowsy driving results in over 71,000 injuries, 1,500 deaths, and
around \$12.5 billion in monetary losses per year. Due to the relevance
of this problem, we believe it is important to develop a solution for
drowsiness detection, especially in the early stages to prevent
accidents.

Additionally, we believe that drowsiness can negatively impact people in
working and classroom environments as well. Although sleep deprivation
and college go hand in hand, drowsiness in the workplace especially
while working with heavy machinery may result in serious injuries
similar to those that occur while driving drowsily.

\textbf{1.1.1 OBJECTIVE}

The objective is to measure physical changes (i.e. open/closed eyes and
mouth to detect fatigue) is well suited for real world conditions to
detect changes. In addition, micro sleeps that are short period of
sleeps lasting 2 to 3 minutes are good indicators of fatigue. Thus, by
continuously monitoring the eyes and mouth of the driver one can detect
the sleepy state of driver.

Our solution to this problem is to build a detection system that
identifies key attributes of drowsiness to detect whether alert or
drowsy.

In this code we build a system of Driver drowsiness detection via eye
and mouth monitoring being it closed or opened using Python, OpenCV.

\textbf{CHAPTER 2}

\textbf{LITERATURE REVIEW}

\textbf{2.1 WHAT IS DROWSINESS?}

Drowsiness is defined as a decreased level of awareness portrayed by
sleepiness and trouble in staying alarm but the person awakes with
simple excitement by stimuli. It might be caused by an absence of rest,
medicine, substance misuse, or a cerebral issue. It is mostly the result
of fatigue which can be both mental and physical. Physical fatigue, or
muscle weariness, is the temporary physical failure of a muscle to
perform ideally. Mental fatigue is a temporary failure to keep up ideal
psychological execution. The onset of mental exhaustion amid any
intellectual action is progressive, and relies on an individual's
psychological capacity, furthermore upon different elements, for
example, lack of sleep and general well-being. Mental exhaustion has
additionally been appeared to diminish physical performance. It can show
as sleepiness, dormancy, or coordinated consideration weakness. In the
past years according to available data driver sleepiness has gotten to
be one of the real reasons for street mishaps prompting demise and
extreme physical injuries and loss of economy. A driver who falls asleep
is in an edge of losing control over the vehicle prompting crash with
other vehicle or stationary bodies. Keeping in mind to stop or reduce
the number of accidents to a great extent the condition of sleepiness of
the driver should be observed continuously.

\textbf{2.2 QUESTIONARIES}

Following information from the driver about more than one incident which
may or may not be critical:

How much sleep did you get before this drive?

What were the road conditions?

What were the traffic conditions?

What type of road was it? E.g. curvy, straight, two-lane, etc.

Were you tired when you started?

What speed were you going?

What time of the day or night was it?

At what point during your drive did this happen?

Where were you going?

How long did you keep driving?

What did you do to try to become more alert?

What else would have helped you become more alert?

After the incident, were you more alert? How long did this ``adrenaline
effect'' last?

Were you more tired once the adrenaline wore off?

When you stopped driving was it because you were drowsy or another
reason?

Did you nap at all when you stopped?

Do you think that you drove at all with your eyes closed? If so, for how
long?

\includegraphics[width=5.475in,height=3.33333in]{media/image2.png}

\includegraphics[width=5.34167in,height=4.29167in]{media/image3.png}

\textbf{CHAPTER 3}

\textbf{COUNTER MEASURES}

The study states that the reason for a mishap can be categorized as one
of the accompanying primary classes:

(1) human

(2) vehicular

(3) surrounding factor

The driver's error represented 91\% of the accidents. The other two
classes of causative elements were referred to as 4\% for the type of
vehicle used and 5\% for surrounding factors. Several measures are
available for the measurement of drowsiness which includes the
following:

\textbf{3.1 Vehicle based measures}

Vehicle-based measures survey path position, which monitors the
vehicle's position as it identifies with path markings, to determine
driver weakness, and accumulate steering wheel movement information to
characterize the fatigue from low level to high level.

\textbf{Disadvantages:}

-Vehicle based measures mostly affected by the geometry of road which
sometimes unnecessarily activates the alarming system.

-The driving style of the current driver needs to be learned and modeled
for the system to be efficient.

-The condition like micro sleeping which mostly happens in straight
highways cannot be detected.

\textbf{3.2 Physiological measures}

Physiological measures are the objective measures of the physical
changes that occur in our body because of fatigue. These physiological
changes are measured by their respective instruments as follows: ECG,
EMG, EOG \& EEG.

\textbf{Disadvantages:}

-It requires placement of several electrodes to be placed on head, chest
and face which is not at all a convenient and annoying for a driver.

-They need to be very carefully placed on respective places for perfect
result.

\textbf{3.3 Subjective Methods}

Subjective methods involve assessing the drivers' current level of
drowsiness by subjecting them to ratings in the form of questionnaires.
These ratings are usually self-evaluated or evaluated by experts
watching the driver in action. To detect the changes in a driver's
drowsiness state, conducted a pre-experimental, mid-experimental, and
postexperimental Karolinska Sleepiness Scale (KSS) exercise. These
ratings were the keys to define their drowsiness ground truth , methods
like Stanford Sleepiness Scale and Karolinska Sleepiness Scale are the
two most widely utilized subjective measures. SSS is a 7-point
measurement scale describing the current state of drowsiness of an
individual which is further most likely be used to categorize driver
drowsiness into only two states. This is because of the close relation
of each scale. KSS, on the other hand, is a 9-point scale. A contrast to
SSS, this scale is considered a robust scale capable of categorizing
driver's drowsiness into different levels.

\textbf{SSS}

\includegraphics[width=6.63333in,height=3.2in]{media/image4.png}

\textbf{KSS}

\includegraphics[width=6.5in,height=3.1827in]{media/image5.png}

\textbf{CHAPTER 4}

\textbf{PROPOSED METHODOLOGY}

\textbf{4.1 BEHAVIORAL MEASURES}

The objective is to measure physical changes (i.e. open/closed eyes and
mouth to detect fatigue) is well suited for real world conditions. Under
this technique eye aspect ratio, mouth aspect ratio, etc. of a person is
monitored and then detect if any of these drowsiness symptoms are
detected.

\textbf{4.2 DATA SOURCE AND PREPROCESSING}

For our training and test data, we use REAL LIFE DROWSINESS DATASET
created by a research team from the University of Texas at Arlington
specifically for detecting multi-stage drowsiness. They had uploaded
data of around 30 hours of videos of 60 unique participants.

The data created by the research team was uploaded in form of zip files,
each file consists of five or six folders in which there are videos of
unique individuals named as 0 for alert state video and 10 for drowsy
state video.

From that dataset, we chose sufficient amount of data for both the alert
and drowsy state of some participants.

For each video, we used OpenCV to extract 1 frame per second starting at
the 3-minute mark until the end of the video.

Each video was approximately 10 minutes long, so we extracted around 240
frames per video.

We labeled the frames from alert videos as 0 and from the drowsy videos
as 1.

There were 68 total landmarks per frame but we decided to keep the
landmarks for the eyes and mouth only (Points 37--68). These were the
important data points we used to extract the features for our model.

\includegraphics[width=5.35in,height=3.85in]{media/image6.png}

\textbf{4.3 FEATURE COMPUTATION}

\textbf{4.3.1 FEATURE EXTRACTION}

We ventured into developing suitable features for our classification
model. While we hypothesized and tested several features like mouth
aspect ratio, pupil circularity, and finally, mouth aspect ratio over
eye aspect ratio, the core features that we concluded on for our final
models were:

\emph{\textbf{Eye Aspect Ratio (EAR)}}

EAR, as the name suggests, is the ratio of the length of the eyes to the
width of the eyes. The length of the eyes is calculated by averaging
over two distinct vertical lines across the eyes as illustrated in the
figure below.

\includegraphics[width=6.5in,height=1.57222in]{media/image7.png}

Our hypothesis was that when an individual is drowsy, their eyes are
likely to get smaller and they are likely to blink more. Based on this
hypothesis, we expected our model to predict the class as drowsy if the
eye aspect ratio for an individual over successive frames started to
decline i.e. their eyes started to be more closed or they were blinking
faster.

\emph{\textbf{Mouth Aspect Ratio (MAR)}}

Computationally similar to the EAR, the MAR, as you would expect,
measures the ratio of the length of the mouth to the width of the mouth.
Our hypothesis was that as an individual becomes drowsy, they are likely
to yawn and lose control over their mouth, making their MAR to be higher
than usual in this state.

\includegraphics[width=6.5in,height=1.92292in]{media/image8.png}

\emph{\textbf{Pupil Circularity (PUC)}}

PUC is a measure complementary to EAR, but it places a greater emphasis
on the pupil instead of the entire eye.

\includegraphics[width=6.5in,height=1.36528in]{media/image9.png}

For example, someone who has their eyes half-open or almost closed will
have a much lower pupil circularity value versus someone who has their
eyes fully open due to the squared term in the denominator. Similar to
the EAR, the expectation was that when an individual is drowsy, their
pupil circularity is likely to decline.

\emph{\textbf{Mouth Over Eye Ratio (MOE)}}

MOE is simply the ratio of the MAR to the EAR.

The benefit of using this feature is that EAR and MAR are expected to
move in opposite directions if the state of the individual changes. As
opposed to both EAR and MAR, MOE as a measure will be more responsive to
these changes as it will capture the subtle changes in both EAR and MAR
and will exaggerate the changes as the denominator and numerator move in
opposite directions. Because the MOE takes MAR as the numerator and EAR
as the denominator, our theory was that as the individual gets drowsy,
the MOE will increase.

\includegraphics[width=5.15in,height=1.16667in]{media/image10.png}

\textbf{4.3.2 FEATURE NORMALIZATION}

When we were testing our models with the core features , whenever we
randomly split the frames in our training and test, our model would
yield results with accuracy as high 70\%, however, whenever we split the
frames by individuals (i.e. an individual that is in the test set will
not be in the training set), our model performance would be poor.

This means our model was struggling with new faces and the primary
reason for this struggle was the fact that each individual has different
core features in their default alert state. That is, person A may
naturally have much smaller eyes than person B. If a model is trained on
person B, the model, when tested on person A, will always predict the
state as drowsy because it will detect a fall in EAR and PUC and a rise
in MOE even though person A was alert. That is why, we hypothesized that
normalizing the features for each individual is likely to yield better
results.

To normalize the features of each individual, we took the first three
frames for each individual's alert video and used them as the baseline
for normalization. The mean and standard deviation of each feature for
these three frames were calculated and used to normalize each feature
individually for each participant. Mathematically, this is what the
normalization equation looked like:

\includegraphics[width=6.5in,height=1.575in]{media/image11.png}

Now that we had normalized each of the four core features, our feature
set had eight features, each core feature complemented by its normalized
version. We tested all eight features in our models and our results
improved significantly.

\textbf{CHAPTER 5}

\textbf{DATA EXPLORATION}

\textbf{5.1 METRICS}

\includegraphics[width=4.08333in,height=2.98333in]{media/image12.png}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  \textbf{VISUALIZATION}
\end{enumerate}

We wanted to get a sense of feature importance so we visualized the
results from our Random Forest model.

Mouth Aspect Ratio after normalization turned out to be the most
important feature out of our 8 features. This makes sense because when
we are drowsy, we tend to yawn more frequently. Normalizing our features
exaggerated this effect and made it a better indicator of drowsiness in
different participants.

\textbf{5.2.1}

\includegraphics[width=5.55in,height=3.11667in]{media/image13.png}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  \textbf{Normalized Feature Value} with \textbf{Time (Frames
  Captured):}
\item
  \textbf{State Prediction} with \textbf{Time (Frames Captured)}
\end{enumerate}

\begin{quote}
For input image captured by webcam:
\end{quote}

\includegraphics[width=6.5in,height=3.95583in]{media/image14.png}

\textbf{CHAPTER 6}

\textbf{SYSTEM DESCRIPTION}

\textbf{6.1 TOOLS USED}

\textbf{1.Python 3 Interpreter}

\textbf{2.OpenCV}

OpenCV is an open source computer vision library which was designed for
computational efficiency and having a high focus on real-time image
detection. One of OpenCV's goals is to provide a simple-to-use computer
vision infrastructure which helps people to build highly sophisticated
vision applications fast. The OpenCV library, containing over 500
functions, spans many areas in vision. Because computer vision and
machine learning often goes hand-in-hand, OpenCV also has a complete,
general-purpose, Machine Learning Library (MLL). This sub library is
focused on statistical pattern recognition and clustering. The MLL is
very useful for the vision functions that are the basis of OpenCV's
usefulness, but is general enough to be used for any machine learning
problem.

\textbf{3.Dlib Libraries and their dependencies}

It is a C++ toolkit for machine learning, it also provides a python API
to use it in your python apps. One of its best features is a great
documentation for C++ and Python API. It is used in the code to detect
faces and get facial landmarks coordinates especially the 12 points
which define the two eyes left and right. After getting the 12 points of
left and right eye, we compute Eye aspect ratio to estimate the level of
the eye opening. Open eyes have high values of EAR while the closed eye
it is getting close to zero.

\textbf{4.Keras --}~To build our classification model ,it uses
TensorFlow as backend.

\textbf{6.2 TECHNOLOGY USED}

The various technology that can be used are discussed as:

\textbf{TensorFlow:}

IT is an open-source software library for dataflow programming across a
range of tasks. It is a symbolic math library, and is also used for
machine learning applications such as neural networks. It is used for
both research and production. TensorFlow computations are expressed as
stateful dataflow graphs. The name TensorFlow derives from the
operations that such neural networks perform on multidimensional data
arrays. These arrays are referred to as "tensors".

\textbf{Machine learning}

Machine learning is the kind of programming which gives computers the
capability to automatically learn from data without being explicitly
programmed. This means in other words that these programs change their
behavior by learning from data. Python is clearly one of the best
languages for machine learning. Python does contain special libraries
for machine learning namely scipy, pandas and numpy which are great for
linear algebra and getting to know kernel methods of machine learning.
The language is great to use when working with machine learning
algorithms and has easy syntax relatively.

\textbf{OpenCV:}

OpenCV is used in applications to easily load bitmap files that contain
landscaping pictures and perform a blend operation between two pictures
so that one picture can be seen in the background of another picture.
This image manipulation is easily performed in a few lines of code using
OpenCV versus other methods. OpenCV.org is a must if you want to explore
and dive deeper into image processing and machine learning in general.

\textbf{CHAPTER 7}

\textbf{ALGORITHMS}

\textbf{7.1 BASIC CLASSIFICATION METHODS}

\textbf{7.1.1 Description}

After we extracted and normalized our features, we wanted to try a
series of modeling techniques, starting with the most basic
classification models like logistic regression and Naive Bayes, moving
on to more complex models containing neural networks and other deep
learning approaches.

~\textbf{To predict the label for each frame in the sequence}

The way we introduced sequence to basic classification methods was to
average the original prediction results with the prediction results from
the previous two frames. Since our dataset was divided into training and
test based on the individual participants and the data points are all in
the order of time sequence, averaging makes sense in this case and
allowed us to deliver more accurate predictions.

\includegraphics[width=5.3in,height=1.96667in]{media/image15.png}

\textbf{7.2 Convolutional Neural Networks (CNN)}

\textbf{7.2.1 Description}

It is a special type of deep neural network which performs extremely
well for image classification purposes. A CNN basically consists of an
input layer, an output layer and a hidden layer which can have multiple
numbers of layers.

Convolutional Neural Networks (CNN) are typically used to analyze image
data and map images to output variables. However, we decided to build a
1-D CNN and send in numerical features as sequential input data to try
and understand the spatial relationship between each feature for the two
states. Our CNN model has 5 layers including 1 convolutional layer, 1
flatten later, 2 fully connected dense layers, and 1 dropout layer
before the output layer. The flatten layer flattens the output from the
convolutional layer and makes it linear before passing it into the first
dense layer. The dropout layer randomly drops 20\% of the output nodes
from the second dense layer in order to prevent our model from
overfitting to the training data. The final dense layer has a single
output node that outputs 0 for alert and 1 for drowsy.

\textbf{7.2.2 CNN PARAMETERS}

\includegraphics[width=5.44167in,height=1.7in]{media/image16.png}

\textbf{7.2.3 CNN MODEL DESIGN:}

\includegraphics[width=6.01667in,height=3.24167in]{media/image17.png}

\textbf{7.3 Long Short-Term Memory Networks}

\textbf{7.3.1 Description}

Another method to deal with sequential data is using an LSTM model. LSTM
networks are a special kind of Recurrent Neural Networks (RNN), capable
of learning long-term dependencies in the data. Recurrent Neural
Networks are feedback neural networks that have internal memory that
allows information to persist.

\textbf{Internal memory space of RNN while processing new data:}

When making a decision, RNNs consider not only the current input but
also the output that it has learned from the previous inputs. This is
also the main difference between RNNs and other neural networks.~In
other neural networks, the inputs are independent of each other. In
RNNs, the inputs are related to each other.

\textbf{Why LSTM?}

We chose to use an LSTM network because it allows us to study long
sequences without having to worry about the gradient vanishing problems
faced by traditional RNNs. Within the LSTM network, there are three
gates for each time step: Forget Gate, Input gate, and Output Gate.

\textbf{Forget Gate}: as its name suggests, the gate tries to ``forget''
part of the memory from the previous output.

\includegraphics[width=3.975in,height=1.15in]{media/image18.png}

\textbf{Input Gate:~}the gate decides what should be kept from the input
in order to modify the memory.

\includegraphics[width=4.10833in,height=1.36667in]{media/image19.png}

\textbf{Output Gate:~}the gate decides what the output is by combining
the input and memory.

\includegraphics[width=3.925in,height=1.24167in]{media/image19.png}

First, we converted our videos into batches of data. Then, each batch
was sent through a fully connected layer with 1024 hidden units using
the sigmoid activation function. The next layer is our LSTM layer with
512 hidden units followed by 3 more FC layers until the final output
layer.

\textbf{7.3.2 LSTM PARAMETERS}

\includegraphics[width=4.70833in,height=0.79167in]{media/image20.png}

\textbf{7.3.3 LSTM MODEL DESIGN}

\includegraphics[width=3.45in,height=4.83333in]{media/image21.png}

\textbf{7.3.4 GRAPHICAL REPRESENTATION OF THE RESULTS:}

\textbf{ROC CURVE \& CALIBRATION CURVE (}Between mean predicted values
and fraction of positives)

\includegraphics[width=5.54167in,height=4.025in]{media/image22.png}

\includegraphics[width=5.36667in,height=3.71667in]{media/image23.png}

\textbf{7.3.5 CONCLUSION}

Our optimized LSTM model achieved an overall accuracy of 77.08\% and a
false-negative rate of 0.3.

\textbf{7.4 COMPARISON OF ALGORITHMS}

\textbf{7.4.1 ACCURACY CURVE}

\includegraphics[width=3.19167in,height=2.64167in]{media/image24.png}

\includegraphics[width=6.5in,height=4.12762in]{media/image25.png}

\textbf{7.4.2 ROC CURVE}

\includegraphics[width=6.20833in,height=3.81667in]{media/image26.png}

\includegraphics[width=6.2in,height=4.325in]{media/image27.png}

\textbf{7.4.3 CALIBRATION CURVE} (Between mean predicted values and
fraction of positives).

\textbf{Results of accuracy when Threshold was at 0.5:}

\includegraphics[width=3.82222in,height=1.89444in]{media/image28.png}

\textbf{Results of accuracy when Threshold lowered from 0.5 to 0.4:}

\includegraphics[width=5.06667in,height=2.40833in]{media/image29.png}

From the different classifiers we tried\textbf{, K-Nearest Neighbor
(KNN, k = 25) had the highest out-of-sample accuracy} of 77.21\%. Naive
Bayes performed the worst at 57.75\% and we concluded that this was
because the model has a harder time dealing with numerical data.
Although KNN yielded the highest accuracy, the false-negative rate was
quite high at 0.42 which means that there is a 42\% probability that
someone who is actually drowsy would be detected as alert by our system.

In order to decrease the false-negative rate, we lowered the threshold
from 0.5 to 0.4 which allowed our model to predict more cases drowsy
than alert. Although the accuracies for some of the other models
increased, KNN still reported the highest accuracy.

\textbf{CHAPTER 8}

\textbf{CONCLUSION}

We concluded quite a few things throughout this project.

First, simpler models can be just as efficient at completing tasks as
more complex models. In our case, the K-Nearest Neighbor model gave an
accuracy similar to the LSTM model. However, because we do not want to
misclassify people who are drowsy as alert, ultimately it is better to
use the more complex model with a lower false-negative rate than a
simpler model that may be cheaper to deploy.

Second, normalization was crucial to our performance. We recognized that
everybody has a different baseline for eye and mouth aspect ratios and
normalizing for each participant was necessary.

\textbf{8.1 LIMITATIONS}

\textbf{Use of spectacles:}

In case the user uses spectacle then it is difficult to detect the state
of the eye. As it hugely depends on light hence reflection of spectacles
may give the output for a closed eye as opened eye. Hence for this
purpose the closeness of eye to the camera is required to avoid light.

\textbf{Multiple face problem:}

If multiple face arises in the window then the camera may detect more
than one faces undesired output may appear. Because of different
condition of different faces. So, we need to make sure that only the
driver face come within the range of the camera. Also, the speed of
detection reduces because of operation on multiple faces.

\textbf{8.2 FUTURE WORK}

There are a few things we can do to further improve our results and
fine-tune the models. First, we need to incorporate distance between the
facial landmarks to account for any movement by the subject in the
video. Realistically the participants will not be static on the screen
and we believe sudden movements by the participant may signal drowsiness
or waking up from micro-sleep.

Also, the response of driver after being warned may not be enough to
stop causing the accident meaning that if the driver is slow in
responding towards the warning signal then accident may occur. Hence to
avoid this we can design and fit a motor driven system and synchronize
it with the warning signal so that the vehicle will slow down after
getting the warning signal automatically.

We can also provide the user with an Android application which will
provide with the information of his/her drowsiness level during any
journey. The user will know Normal state, Drowsy State, the number of
times blinked the eyes according to the number of frames captures
